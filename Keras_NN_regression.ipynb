{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "elec.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh30pZSZZXfh"
      },
      "source": [
        "# imports\n",
        "import pandas \n",
        "import numpy \n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.optimizers import Adam\n",
        "from matplotlib import pyplot\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTBvxWaKy_lD"
      },
      "source": [
        "# load data\n",
        "!curl -L -o table.csv \"https://www.eia.gov/consumption/residential/data/2009/csv/recs2009_public.csv\"\n",
        "df = pandas.read_csv(\"table.csv\")\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mayOEDk0HauA"
      },
      "source": [
        "\"\"\"\n",
        "Data preproccessing class: inclues different methods for data cleaning, feature engineering and feature scaling.\n",
        "\"\"\"\n",
        "class data_preproccessing:\n",
        "\n",
        "  # initialization method\n",
        "  def __init__(self, data):\n",
        "    self.dataFrame = data\n",
        "\n",
        "  # seprates for saprating labels from features \n",
        "  def seprate_labels(self):\n",
        "    labels = self.dataFrame['Price']\n",
        "    self.dataFrame = self.dataFrame.drop('Price', axis=1)\n",
        "    return labels\n",
        "\n",
        "  # removes redundant features from data\n",
        "  def redundant_feature(self, features):\n",
        "    self.dataFrame = self.dataFrame.drop(features, axis=1)\n",
        "\n",
        "  # Splits data into training set and test sets \n",
        "  def split_data(self):\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(self.dataFrame, self.seprate_labels(), test_size = 0.20)\n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "  # It imputes missing values by a new level or by the 'mean' of the feature\n",
        "  def missing_values(self):\n",
        "    strings = self.dataFrame.select_dtypes(include=['object']).columns\n",
        "    nonstrings = self.dataFrame.select_dtypes(exclude=['object']).columns\n",
        "    for feature in strings:\n",
        "      print(feature, ': # of missing values:',self.dataFrame[feature].isnull().sum())\n",
        "      self.dataFrame[feature].fillna('U')\n",
        "    for feature in nonstrings:\n",
        "      print(feature, ': # of missing values:',self.dataFrame[feature].isnull().sum())\n",
        "      self.dataFrame[feature].fillna(self.dataFrame[feature].mean(), inplace=True)\n",
        "\n",
        "  # Applies one-hot encoding for the nomials features \n",
        "  def one_hot_encoding(self, nomial_features):\n",
        "    for feature in nomial_features:\n",
        "      one_hot = pd.get_dummies(self.dataFrame[feature], prefix=feature)\n",
        "      self.dataFrame = self.dataFrame.drop(feature,axis = 1)\n",
        "      self.dataFrame = self.dataFrame.join(one_hot)\n",
        "  \n",
        "  # Converts ordinal features to number \n",
        "  def conver_to_num(self, nomial_features):\n",
        "    for feature in nomial_features:\n",
        "      self.dataFrame[feature] = self.dataFrame[feature].astype('category').cat.codes   \n",
        "\n",
        "  # Converts 'Date' feature to datatime type and defines a new feature called 'Age'\n",
        "  # Age of a house is the difference between it's sold and the date it's built\n",
        "  # Drops 'YearBuilt' as it does not matter once we have the age of the house\n",
        "  def hanlde_Date(self):\n",
        "    # parse date values seprated with slashes as datetime\n",
        "    self.dataFrame['Date'] = pd.to_datetime(self.dataFrame.Date)\n",
        "    self.dataFrame['Age'] = self.dataFrame['Date'].dt.year - self.dataFrame['YearBuilt']\n",
        "    self.dataFrame = self.dataFrame.drop('YearBuilt', axis=1)\n",
        "    self.dataFrame[\"Date\"] = self.dataFrame[\"Date\"].astype('category').cat.codes \n",
        "    self.dataFrame[\"Age\"] = self.dataFrame[\"Age\"].astype('category').cat.codes \n",
        "\n",
        "  # Replaces the values of 'Postcode' feature with mean of Price for each level\n",
        "  def hanlde_Postcode(self):\n",
        "    for val in housing_prices.Postcode.unique():\n",
        "      mean = self.dataFrame.loc[self.dataFrame['Postcode'] == val, 'Price'].mean()\n",
        "      self.dataFrame['Postcode'] = self.dataFrame['Postcode'].replace([val],[mean])\n",
        "\n",
        "  # Represent latitudes and longitudes via 3 coordinates\n",
        "  def handle_Lat_Long(self):\n",
        "    self.dataFrame[\"x\"] = np.sin(np.deg2rad(self.dataFrame['Lattitude']))*np.cos(np.deg2rad(self.dataFrame['Longtitude']))\n",
        "    self.dataFrame[\"y\"] = np.cos(np.deg2rad(self.dataFrame['Lattitude']))*np.sin(np.deg2rad(self.dataFrame['Longtitude']))\n",
        "    self.dataFrame[\"z\"] = np.sin(np.deg2rad(self.dataFrame['Lattitude']))  \n",
        "    self.dataFrame = self.dataFrame.drop('Lattitude', axis=1)  \n",
        "    self.dataFrame = self.dataFrame.drop('Longtitude', axis=1)\n",
        "    self.dataFrame = self.dataFrame.drop('Landsize', axis=1)\n",
        "\n",
        "  # inspect the importance of each feature\n",
        "  def feature_importance(self):\n",
        "    regressor = DecisionTreeRegressor(random_state=0)\n",
        "    X_train, X_test, Y_train, Y_test = split_data(self.dataFrame)\n",
        "    regressor.fit(X_train, Y_train)\n",
        "    print(regressor.feature_importances_)\n",
        "\n",
        "  # performs data normalization \n",
        "  def normalization(self):\n",
        "    X_train, X_test, Y_train, Y_test = self.split_data()\n",
        "    X_train_norm = X_train.copy()\n",
        "    X_test_norm = X_test.copy()\n",
        "    norm = MinMaxScaler().fit(X_train_norm)\n",
        "    X_train_norm = norm.transform(X_train_norm)\n",
        "    X_test_norm = norm.transform(X_test_norm) \n",
        "    return X_train_norm, X_test_norm, Y_train, Y_test\n",
        "\n",
        "  # performs data standization \n",
        "  def standization(self):\n",
        "    X_train, X_test, Y_train, Y_test = self.split_data()\n",
        "    X_train_stand = X_train.copy()\n",
        "    X_test_stand = X_test.copy()\n",
        "    norm = StandardScaler().fit(X_train_stand)\n",
        "    X_train_stand = norm.transform(X_train_stand)\n",
        "    X_test_stand = norm.transform(X_test_stand) \n",
        "    return X_train_stand, X_test_stand, Y_train, Y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A-s3nH9sDl3"
      },
      "source": [
        "# build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(30, activation=\"relu\", input_dim=456))\n",
        "model.add(Dense(20, activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"linear\"))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer=Adam(lr=1e-3, decay=1e-3/100))\n",
        "\n",
        "# fit the model\n",
        "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=50, batch_size=500, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quGAogEiz2qc"
      },
      "source": [
        "# calculate predictions\n",
        "pred_test_set = model.predict(X_test)\n",
        "\n",
        "# save predictions\n",
        "# numpy.savetxt(\"test_results.csv\", pred_test_set, delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFq2mpnuUhYB"
      },
      "source": [
        "# plot the training history\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "plt.title('model loss')\n",
        "plt.xlabel('epoch number')\n",
        "plt.ylabel('total loss for all samples')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBFEqXkMqTjD"
      },
      "source": [
        "# plot the predicted values vs the actual values \n",
        "# test_results = numpy.genfromtxt(\"test_results.csv\", delimiter=\",\")\n",
        "plt.plot(Y_test,pred_test_set,'bo')\n",
        "plt.title('Test Set')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "\n",
        "# compute R-Square value for test set\n",
        "TestR2Value = r2_score(Y_test,pred_test_set)\n",
        "print(\"Test Set R-Square=\", round(TestR2Value, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqw2WCoupTH8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}